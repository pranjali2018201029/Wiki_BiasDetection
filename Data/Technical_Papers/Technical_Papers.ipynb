{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "class CoreApiRequestor:\n",
    "\n",
    "    def __init__(self, endpoint, api_key):\n",
    "        self.endpoint = endpoint\n",
    "        self.api_key = api_key\n",
    "        #defaults\n",
    "        self.pagesize = 100\n",
    "        self.page = 1\n",
    "\n",
    "    def parse_response(self, decoded):\n",
    "        res = []\n",
    "        for item in decoded['data']:\n",
    "            doi = None\n",
    "            if 'identifiers' in item:\n",
    "                for identifier in item['identifiers']:\n",
    "                    if identifier and identifier.startswith('doi:'):\n",
    "                        doi = identifier\n",
    "                        break\n",
    "            res.append([item['title'], doi])\n",
    "        return res\n",
    "\n",
    "    def request_url(self, url):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html = response.read()\n",
    "        return html\n",
    "\n",
    "    def get_method_query_request_url(self,method,query,fullText,page):\n",
    "        if (fullText):\n",
    "            fullText = 'true'\n",
    "        else:\n",
    "            fullText = 'false'\n",
    "        params = {\n",
    "            'apiKey':self.api_key,\n",
    "            'page':page,\n",
    "            'pageSize':self.pagesize,\n",
    "            'fulltext':fullText\n",
    "        }\n",
    "        return self.endpoint + method + '/' + urllib.parse.quote(query) + '?' + urllib.parse.urlencode(params)\n",
    "\n",
    "    def get_up_to_20_pages_of_query(self,method,query,fulltext):\n",
    "        url = self.get_method_query_request_url(method,query,fulltext,1)\n",
    "        all_articles=[]\n",
    "        resp = self.request_url(url)\n",
    "        result = json.loads(resp.decode('utf-8'))\n",
    "        all_articles.append(result)\n",
    "        if (result['totalHits']>100):\n",
    "            numOfPages = int(result['totalHits']/self.pagesize)  #rounds down\n",
    "            if (numOfPages>20):\n",
    "                numOfPages=20\n",
    "            for i in range(2,numOfPages):\n",
    "                url = self.get_method_query_request_url(method,query,False,i)\n",
    "                print(url)\n",
    "                resp =self.request_url(url)\n",
    "                all_articles.append(json.loads(resp.decode('utf-8')))\n",
    "        return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "Dir_Path = \"/Users/pranjali/Downloads/Wiki_BiasDetection/Data/Technical_Papers/Papers_Data/\"\n",
    "endpoint = 'https://core.ac.uk/api-v2'\n",
    "method = '/articles/search'\n",
    "api_key = 'zXFb53YjLQ8f0Kw7V6H4nqMPtpcgyuNC'\n",
    "api = CoreApiRequestor(endpoint,api_key)\n",
    "\n",
    "topics = ['blockchain', 'deep AND learning', 'quantum AND physics', 'inorganic AND chemistry', 'psychology', 'astrophysics',\n",
    "         'group AND theory', 'molecular AND networks', 'computation AND language', 'economics', 'signal AND processing']\n",
    "FullText = False\n",
    "\n",
    "if not os.path.isdir(Dir_Path):\n",
    "    os.mkdir(Dir_Path)\n",
    "\n",
    "for topic in topics:\n",
    "\n",
    "    url = api.get_method_query_request_url(method,topic,FullText,1)\n",
    "    # print(\"url: \", url)\n",
    "\n",
    "    result = api.request_url(url)\n",
    "    result_str = result.decode('utf8')\n",
    "    data = json.loads(result_str)\n",
    "     \n",
    "    with open(Dir_Path + topic + '.json', 'w') as file:\n",
    "        json.dump(data, file)\n",
    "        \n",
    "    columns = ['id', 'publisher', 'description']\n",
    "    topic_papers = []\n",
    "#     topic_papers.append(columns)\n",
    "    \n",
    "    for paper in data['data']:\n",
    "        \n",
    "        paper_row = []\n",
    "        \n",
    "        if 'id' in paper.keys():\n",
    "            paper_row.append(paper['id'])\n",
    "            \n",
    "            if ('publisher' in paper.keys()):\n",
    "                paper_row.append(paper['publisher'])\n",
    "            else:\n",
    "                paper_row.append(\"NA\")\n",
    "\n",
    "            if ('description' in paper.keys()):\n",
    "                paper_row.append(paper['description'])\n",
    "            else:\n",
    "                paper_row.append(\"NA\")\n",
    "\n",
    "            topic_papers.append(paper_row)\n",
    "        \n",
    "        df = pd.DataFrame(topic_papers)\n",
    "        \n",
    "        df.to_csv(Dir_Path + topic + '.csv', header=columns)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
