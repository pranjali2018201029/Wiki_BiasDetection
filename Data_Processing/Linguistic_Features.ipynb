{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encodings\n",
    "\n",
    "## POS Tags\n",
    "## For descriptions of these tags refer: https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "POS_Tag_Encoding = {\"CC\" : 0, \"CD\" : 1, \"DT\" : 2, \"EX\" : 3, \"FW\" : 4, \"IN\" : 5, \"JJ\" : 6, \"JJR\" : 7, \"JJS\" : 8,\n",
    "                   \"LS\" : 9, \"MD\" : 10, \"NN\" : 11, \"NNS\" : 12, \"NNP\" : 13, \"NNPS\" : 14, \"PDT\" : 15, \"POS\" : 16,\n",
    "                   \"PRP\" : 17, \"PRP$\" : 18, \"RB\" : 19, \"RBR\" : 20, \"RBS\" : 21, \"RP\" : 22, \"TO\" : 23, \"UH\" : 24,\n",
    "                   \"VB\" : 25, \"VBD\" : 26, \"VBG\" : 27, \"VBN\" : 28, \"VBP\" : 29, \"VBZ\" : 30, \"WDT\" : 31, \"WP\" : 32,\n",
    "                   \"WP$\" : 33, \"WRB\" : 34}\n",
    "\n",
    "## Position in sentence\n",
    "Sent_Position_Encoding = {\"Start\" : 0, \"Middle\" : 1, \"End\" : 2 }\n",
    "\n",
    "## Polarity\n",
    "Polarity_Encoding = {\"Negative\" : 0, \"Positive\" : 1, \"Neutral\" : 2}\n",
    "\n",
    "## Gramatical relation : Not Implemented\n",
    "# Grammatical_Rel_Encoding = {}\n",
    "\n",
    "## Hedge, Factive Verb, Assertive Verb, Implicative Verb, Report Verb, Entailment, Strong Subjective, Weak Subjective,\n",
    "## Positive word, Negative Word, Bias Lexicon\n",
    "## For all above : 1 --> Present in the corresponding lexicon list, 0 --> Not present in the corresponding lexicon list\n",
    "Hedges = []\n",
    "Factive_Verbs = []\n",
    "Assertive_Verbs = []\n",
    "Implicative_Verbs = []\n",
    "Report_Verbs = []\n",
    "Entailments = []\n",
    "Strong_Subjectives = []\n",
    "Weak_Subjectives = []\n",
    "Positive_Words = []\n",
    "Negative_Words = []\n",
    "Bias_Lexicons = []\n",
    "\n",
    "Lexicon_Folder_Path = \"/Users/pranjali/Downloads/Wiki_BiasDetection/Lexicons/\"\n",
    "\n",
    "Feature_Columns = [\"word\", \"lemma\", \"POS\", \"POS_Prev\", \"POS_Next\", \"Sent_Position\", \n",
    "                  \"Hedge\", \"Hedge_Context\", \"Factive\", \"Factive_Context\", \"Assertive\", \"Assertive_Context\",\n",
    "                  \"Implicative\", \"Implicative_Context\", \"Report\", \"Report_Context\", \n",
    "                   \"Entailment\", \"Entailment_Context\", \"StrongSub\", \"StrongSub_Context\", \n",
    "                   \"WeakSub\", \"WeakSub_Context\", \"Polarity\", \"Positive\", \"Positive_Context\", \n",
    "                   \"Negative\", \"Negative_Context\", \"Bias_Lexicon\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Load_Lexicon_File(Filename):\n",
    "    \n",
    "    with open(Lexicon_Folder_Path + Filename, \"r\") as file:\n",
    "        Lines = file.readlines()\n",
    "    \n",
    "    Lexicon_List = []\n",
    "    for line in Lines:\n",
    "        Lexicon_List.append(line[:-1])\n",
    "        \n",
    "    return Lexicon_List\n",
    "\n",
    "\n",
    "def Load_Lexicons():\n",
    "    \n",
    "    global Hedges, Factive_Verbs, Assertive_Verbs, Implicative_Verbs, Report_Verbs, Entailments\n",
    "    global Strong_Subjectives, Weak_Subjectives, Positive_Words, Negative_Words, Bias_Lexicons\n",
    "    \n",
    "    Hedges = Load_Lexicon_File(\"Hyland_Hedges_2005.txt\")\n",
    "    Factive_Verbs = Load_Lexicon_File(\"Hooper_Factives_1975.txt\")\n",
    "    Assertive_Verbs = Load_Lexicon_File(\"Hooper_Assertives_1975.txt\")\n",
    "    Implicative_Verbs = Load_Lexicon_File(\"Karttunen_Implicatives_1971.txt\")\n",
    "    Report_Verbs = Load_Lexicon_File(\"Report_Verbs.txt\")\n",
    "    Entailments = Load_Lexicon_File(\"Berant_Entailments_2012.txt\")\n",
    "    Strong_Subjectives = Load_Lexicon_File(\" Wiebe_Riloff_Strong_Subjectives_2003.txt\")\n",
    "    Weak_Subjectives = Load_Lexicon_File(\"Wiebe_Riloff_Weak_Subjectives_2003.txt\")\n",
    "    Positive_Words = Load_Lexicon_File(\"Liu_Positive_Words_2005.txt\")\n",
    "    Negative_Words = Load_Lexicon_File(\"Liu_Negative_Words_2005.txt\")\n",
    "    Bias_Lexicons = Load_Lexicon_File(\"NPOV_Edits.txt\")\n",
    "\n",
    "## As lemma is a string value, not sure how to use it ( Can't use its embedding, \n",
    "## Can we calculate other  linguistic features on lemma instead of original word?)\n",
    "def Get_Lemma(word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma\n",
    "\n",
    "\n",
    "## Input: Sentence, Output: List of tuples (word, POS_Tag) for all words in the sentence\n",
    "def Get_POS_Tag(sentence):\n",
    "    sent_words = word_tokenize(sentence)\n",
    "    POS_Tag_List = nltk.pos_tag(sent_words)\n",
    "    return POS_Tag_List\n",
    "\n",
    "\n",
    "## Returns position of the word in the sentence. (Start->0, Middle->1, End->2)\n",
    "def Get_Sentence_Position(word, sentence):\n",
    "    \n",
    "    sent_words = word_tokenize(sentence)\n",
    "    part_size = int(len(sent_words)/3)\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        start_part = i*part_size  \n",
    "        if i==2:\n",
    "            part_size = len(sent_words) - (2*part_size)\n",
    "        for j in range(0,part_size):\n",
    "            if sent_words[start_part+j] == word:\n",
    "                return i\n",
    "\n",
    "## Ideally, The polarity of word according to Riloff and Wiebe's paper need to be calculated\n",
    "## But I have used TextBlob.\n",
    "def Get_Polarity(word):\n",
    "    \n",
    "    Polarity = TextBlob(word).sentiment.polarity\n",
    "    \n",
    "    if Polarity < 0:\n",
    "        Polarity_Val = 0\n",
    "    elif Polarity > 0:\n",
    "        Polarity_Val = 1\n",
    "    else:\n",
    "        Polarity_Val = 2\n",
    "        \n",
    "    return Polarity_Val\n",
    "\n",
    "\n",
    "## Not implemented yet\n",
    "def Get_Grammatical_Rel(word, sentence):\n",
    "    return GR_Val\n",
    "\n",
    "\n",
    "## Not implemented yet : We can use NPOV edits data that we have \n",
    "def Get_Collaborative_Feature(word):\n",
    "    return CF_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input : Sentence\n",
    "## Output : Pandas DataFrame where each row represents linguistic features of word in the sentence, \n",
    "##          Columns names list is mentioned at the start of the code\n",
    "\n",
    "def Get_Sent_Linguistic_Features(Sentence):\n",
    "    \n",
    "    ## Word_Features: List representing linguistic features of the word in the sentence\n",
    "    ## Sentence_Features : List of Word_Features lists \n",
    "    \n",
    "    Sentence_Features = []\n",
    "    \n",
    "    Sent_Words = word_tokenize(Sentence)\n",
    "    Sent_Words = [w for w in Sent_Words if w.isalpha()]\n",
    "    Sent_Length = len(Sent_Words)\n",
    "    \n",
    "    Sent_POS_Tags = Get_POS_Tag(Sentence)\n",
    "    Sent_POS_Tags = [t for t in Sent_POS_Tags if t[0].isalpha()]\n",
    "    \n",
    "    for Word_Index in range(len(Sent_Words)):\n",
    "        \n",
    "        Word = Sent_Words[Word_Index]\n",
    "        \n",
    "        ## Feature 1: Word (string)\n",
    "        Word_Features = [Word]\n",
    "\n",
    "        ## Feature 2: Lemma (string)\n",
    "        word_lemma = Get_Lemma(Word)\n",
    "        Word_Features.append(word_lemma)\n",
    "\n",
    "        ## Feature 3: POS Tag (Tag encoded into int)\n",
    "        POS_Tag = Sent_POS_Tags[Word_Index][1]\n",
    "        POS_Tag_Val = POS_Tag_Encoding[POS_Tag]\n",
    "        Word_Features.append(POS_Tag_Val)\n",
    "\n",
    "        ## Feature 4: POS Tag of previous word (Tag encoded into int)\n",
    "        if Word_Index > 0:\n",
    "            POS_Tag = Sent_POS_Tags[Word_Index-1][1]\n",
    "            POS_Tag_Val = POS_Tag_Encoding[POS_Tag]\n",
    "        else:\n",
    "            POS_Tag_Val = -1\n",
    "        Word_Features.append(POS_Tag_Val)\n",
    "\n",
    "        ## Feature 5: POS Tag of next word (Tag encoded into int)\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            POS_Tag = Sent_POS_Tags[Word_Index+1][1]\n",
    "            POS_Tag_Val = POS_Tag_Encoding[POS_Tag]\n",
    "        else:\n",
    "            POS_Tag_Val = -1\n",
    "        Word_Features.append(POS_Tag_Val)\n",
    "\n",
    "        ## Feature 6: Position of word in the sentence (Encoded into int)\n",
    "        Sent_Position_Val = Get_Sentence_Position(Word, Sentence)\n",
    "        Word_Features.append(Sent_Position_Val)\n",
    "\n",
    "        ## Feature 7: Hedge (1 or 0)\n",
    "        if Word in Hedges:\n",
    "            Hedge_Val = 1\n",
    "        else:\n",
    "            Hedge_Val = 0\n",
    "        Word_Features.append(Hedge_Val)\n",
    "\n",
    "        ## Feature 8: Hedge Context i.e. if Hedge is present in the context (1 or 0)\n",
    "        Prev_Hedge_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Hedges:\n",
    "                Prev_Hedge_Val = 1\n",
    "        \n",
    "        Next_Hedge_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Hedges:\n",
    "                Next_Hedge_Val = 1\n",
    "                \n",
    "        if Prev_Hedge_Val or Next_Hedge_Val:\n",
    "            Hedge_Val = 1\n",
    "        else: \n",
    "            Hedge_Val = 0\n",
    "            \n",
    "        Word_Features.append(Hedge_Val)\n",
    "\n",
    "        ## Feature 9: Factive Verb (1 or 0)\n",
    "        if Word in Factive_Verbs:\n",
    "            Factive_Verb_Val = 1\n",
    "        else:\n",
    "            Factive_Verb_Val = 0\n",
    "        Word_Features.append(Factive_Verb_Val)\n",
    "\n",
    "        ## Feature 10: Factive Verb Context i.e. if Factive Verb is present in the context (1 or 0)\n",
    "        Prev_Factive_Verb_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Factive_Verbs:\n",
    "                Prev_Factive_Verb_Val = 1\n",
    "        \n",
    "        Next_Factive_Verb_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Factive_Verbs:\n",
    "                Next_Factive_Verb_Val = 1\n",
    "                \n",
    "        if Prev_Factive_Verb_Val or Next_Factive_Verb_Val:\n",
    "            Factive_Verb_Val = 1\n",
    "        else: \n",
    "            Factive_Verb_Val = 0\n",
    "            \n",
    "        Word_Features.append(Factive_Verb_Val)\n",
    "        \n",
    "        ## Feature 11: Assertive Verb (1 or 0)\n",
    "        if Word in Assertive_Verbs:\n",
    "            Assertive_Verb_Val = 1\n",
    "        else:\n",
    "            Assertive_Verb_Val = 0\n",
    "        Word_Features.append(Assertive_Verb_Val)\n",
    "\n",
    "        ## Feature 12: Assertive Verb Context i.e. if Assertive Verb is present in the context (1 or 0)\n",
    "        Prev_Assertive_Verb_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Assertive_Verbs:\n",
    "                Prev_Assertive_Verb_Val = 1\n",
    "        \n",
    "        Next_Assertive_Verb_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Assertive_Verbs:\n",
    "                Next_Assertive_Verb_Val = 1\n",
    "                \n",
    "        if Prev_Assertive_Verb_Val or Next_Assertive_Verb_Val:\n",
    "            Assertive_Verb_Val = 1\n",
    "        else: \n",
    "            Assertive_Verb_Val = 0\n",
    "            \n",
    "        Word_Features.append(Assertive_Verb_Val)\n",
    "        \n",
    "        ## Feature 13: Implicative Verb (1 or 0)\n",
    "        if Word in Implicative_Verbs:\n",
    "            Implicative_Verb_Val = 1\n",
    "        else:\n",
    "            Implicative_Verb_Val = 0\n",
    "        Word_Features.append(Implicative_Verb_Val)\n",
    "\n",
    "        ## Feature 14: Implicative Verb Context i.e. if Implicative Verb is present in the context (1 or 0)\n",
    "        Prev_Implicative_Verb_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Implicative_Verbs:\n",
    "                Prev_Implicative_Verb_Val = 1\n",
    "        \n",
    "        Next_Implicative_Verb_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Implicative_Verbs:\n",
    "                Next_Implicative_Verb_Val = 1\n",
    "                \n",
    "        if Prev_Implicative_Verb_Val or Next_Implicative_Verb_Val:\n",
    "            Implicative_Verb_Val = 1\n",
    "        else: \n",
    "            Implicative_Verb_Val = 0\n",
    "            \n",
    "        Word_Features.append(Implicative_Verb_Val)\n",
    "        \n",
    "        ## Feature 15: Report Verb (1 or 0)\n",
    "        if Word in Report_Verbs:\n",
    "            Report_Verb_Val = 1\n",
    "        else:\n",
    "            Report_Verb_Val = 0\n",
    "        Word_Features.append(Report_Verb_Val)\n",
    "\n",
    "        ## Feature 16: Report Verb Context i.e. if Report Verb is present in the context (1 or 0)\n",
    "        Prev_Report_Verb_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Report_Verbs:\n",
    "                Prev_Report_Verb_Val = 1\n",
    "        \n",
    "        Next_Report_Verb_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Report_Verbs:\n",
    "                Next_Report_Verb_Val = 1\n",
    "                \n",
    "        if Prev_Report_Verb_Val or Next_Report_Verb_Val:\n",
    "            Reporte_Verb_Val = 1\n",
    "        else: \n",
    "            Report_Verb_Val = 0\n",
    "            \n",
    "        Word_Features.append(Report_Verb_Val)\n",
    "        \n",
    "        ## Feature 17: Entailment (1 or 0)\n",
    "        if Word in Entailments:\n",
    "            Entailment_Val = 1\n",
    "        else:\n",
    "            Entailment_Val = 0\n",
    "        Word_Features.append(Entailment_Val)\n",
    "\n",
    "        ## Feature 18: Entailment Context i.e. if Entailment is present in the context (1 or 0)\n",
    "        Prev_Entailment_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Entailments:\n",
    "                Prev_Entailment_Val = 1\n",
    "        \n",
    "        Next_Entailment_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Entailments:\n",
    "                Next_Entailment_Val = 1\n",
    "                \n",
    "        if Prev_Entailment_Val or Next_Entailment_Val:\n",
    "            Entailment_Val = 1\n",
    "        else: \n",
    "            Entailment_Val = 0\n",
    "            \n",
    "        Word_Features.append(Entailment_Val)\n",
    "        \n",
    "        ## Feature 19: Strong Subjective (1 or 0)\n",
    "        if Word in Strong_Subjectives:\n",
    "            Strong_Subjective_Val = 1\n",
    "        else:\n",
    "            Strong_Subjective_Val = 0\n",
    "        Word_Features.append(Strong_Subjective_Val)\n",
    "\n",
    "        ## Feature 20: Strong Subjective Context i.e. if Strong Subjective is present in the context (1 or 0)\n",
    "        Prev_Strong_Subjective_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Strong_Subjectives:\n",
    "                Prev_Strong_Subjective_Val = 1\n",
    "        \n",
    "        Next_Strong_Subjective_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Strong_Subjectives:\n",
    "                Next_Strong_Subjective_Val = 1\n",
    "                \n",
    "        if Prev_Strong_Subjective_Val or Next_Strong_Subjective_Val:\n",
    "            Strong_Subjective_Val = 1\n",
    "        else: \n",
    "            Strong_Subjective_Val = 0\n",
    "            \n",
    "        Word_Features.append(Strong_Subjective_Val)\n",
    "        \n",
    "        ## Feature 21: Weak Subjective (1 or 0)\n",
    "        if Word in Weak_Subjectives:\n",
    "            Weak_Subjective_Val = 1\n",
    "        else:\n",
    "            Weak_Subjective_Val = 0\n",
    "        Word_Features.append(Weak_Subjective_Val)\n",
    "\n",
    "        ## Feature 22: Weak Subjective Context i.e. if Weak Subjective is present in the context (1 or 0)\n",
    "        Prev_Weak_Subjective_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Weak_Subjectives:\n",
    "                Prev_Weak_Subjective_Val = 1\n",
    "        \n",
    "        Next_Weak_Subjective_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Weak_Subjectives:\n",
    "                Next_Weak_Subjective_Val = 1\n",
    "                \n",
    "        if Prev_Weak_Subjective_Val or Next_Weak_Subjective_Val:\n",
    "            Weak_Subjective_Val = 1\n",
    "        else: \n",
    "            Weak_Subjective_Val = 0\n",
    "            \n",
    "        Word_Features.append(Weak_Subjective_Val)\n",
    "        \n",
    "        ## Feature 23: Polarity of the word (0, 1, 2) \n",
    "        Polarity_Val = Get_Polarity(Word)\n",
    "        Word_Features.append(Weak_Subjective_Val)\n",
    "        \n",
    "        ## Feature 24: Positive Word (1 or 0)\n",
    "        if Word in Positive_Words:\n",
    "            Positive_Word_Val = 1\n",
    "        else:\n",
    "            Positive_Word_Val = 0\n",
    "        Word_Features.append(Positive_Word_Val)\n",
    "\n",
    "        ## Feature 25: Positive Word Context i.e. if Positive Word is present in the context (1 or 0)\n",
    "        Prev_Positive_Word_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Positive_Words:\n",
    "                Prev_Positive_Word_Val = 1\n",
    "        \n",
    "        Next_Positive_Word_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Positive_Words:\n",
    "                Next_Positive_Word_Val = 1\n",
    "                \n",
    "        if Prev_Positive_Word_Val or Next_Positive_Word_Val:\n",
    "            Positive_Word_Val = 1\n",
    "        else: \n",
    "            Positive_Word_Val = 0\n",
    "            \n",
    "        Word_Features.append(Positive_Word_Val)\n",
    "        \n",
    "        ## Feature 26: Negative Word (1 or 0)\n",
    "        if Word in Negative_Words:\n",
    "            Negative_Word_Val = 1\n",
    "        else:\n",
    "            Negative_Word_Val = 0\n",
    "        Word_Features.append(Negative_Word_Val)\n",
    "\n",
    "        ## Feature 27: Negative Word Context i.e. if Negative Word is present in the context (1 or 0)\n",
    "        Prev_Negative_Word_Val = 0\n",
    "        if Word_Index > 0:\n",
    "            if Word in Negative_Words:\n",
    "                Prev_Negative_Word_Val = 1\n",
    "        \n",
    "        Next_Negative_Word_Val = 0\n",
    "        if Word_Index < Sent_Length-1:\n",
    "            if Word in Negative_Words:\n",
    "                Next_Negative_Word_Val = 1\n",
    "                \n",
    "        if Prev_Negative_Word_Val or Next_Negative_Word_Val:\n",
    "            Negative_Word_Val = 1\n",
    "        else: \n",
    "            Negative_Word_Val = 0\n",
    "            \n",
    "        Word_Features.append(Negative_Word_Val)\n",
    "        \n",
    "        ## Feature 28: Bias Lexicon (1 or 0)\n",
    "        if Word in Bias_Lexicons:\n",
    "            Negative_Word_Val = 1\n",
    "        else:\n",
    "            Negative_Word_Val = 0\n",
    "        Word_Features.append(Negative_Word_Val)\n",
    "        \n",
    "        \n",
    "        ## Add Word feature vector to Sentence_Features\n",
    "        Sentence_Features.append(Word_Features)\n",
    "        \n",
    "    Sentence_Features_DF = pd.DataFrame(Sentence_Features, columns = Feature_Columns)\n",
    "    return Sentence_Features_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 28)\n",
      "             word          lemma  POS  POS_Prev  POS_Next  Sent_Position  \\\n",
      "0              it             it   17        -1        26              0   \n",
      "1             was             wa   26        17        19              0   \n",
      "2          rather         rather   19        26         6              0   \n",
      "3     unfortunate    unfortunate    6        19         5              0   \n",
      "4            that           that    5         6        17              0   \n",
      "5              he             he   17         5        19              0   \n",
      "6      vehemently     vehemently   19        17        26              0   \n",
      "7         opposed        opposed   26        19         2              0   \n",
      "8             the            the    2        26        11              0   \n",
      "9         budding        budding   11         2         6              0   \n",
      "10         indian         indian    6        11        11              0   \n",
      "11      scientist      scientist   11         6         6              0   \n",
      "12   subrahmanyan   subrahmanyan    6        11        11              0   \n",
      "13  chandrasekhar  chandrasekhar   11         6         5              0   \n",
      "14          about          about    5        11        18              0   \n",
      "15            his            his   18         5        11              1   \n",
      "16         theory         theory   11        18         5              1   \n",
      "17             on             on    5        11         2              1   \n",
      "18            the            the    2         5         6              0   \n",
      "19        maximum        maximum    6         2        11              1   \n",
      "20           mass           mass   11         6         5              1   \n",
      "21             of             of    5        11        12              1   \n",
      "22          stars           star   12         5        28              1   \n",
      "23          known          known   28        12         5              1   \n",
      "24             as              a    5        28         6              1   \n",
      "25          white          white    6         5        11              1   \n",
      "26         dwarfs          dwarf   11         6         2              1   \n",
      "27            the            the    2        11        11              0   \n",
      "28           mass           mass   11         2         5              1   \n",
      "29          above          above    5        11        31              2   \n",
      "30          which          which   31         5         2              2   \n",
      "31            the            the    2        31        11              0   \n",
      "32           star           star   11         2        12              2   \n",
      "33      collapses       collapse   12        11         0              2   \n",
      "34            and            and    0        12        30              2   \n",
      "35        becomes        becomes   30         0         2              2   \n",
      "36              a              a    2        30         6              2   \n",
      "37        neutron        neutron    6         2        11              2   \n",
      "38           star           star   11         6         6              2   \n",
      "39          quark          quark    6        11        11              2   \n",
      "40           star           star   11         6         0              2   \n",
      "41             or             or    0        11         6              2   \n",
      "42          black          black    6         0        11              2   \n",
      "43           hole           hole   11         6        -1              2   \n",
      "\n",
      "    Hedge  Hedge_Context  Factive  Factive_Context      ...       StrongSub  \\\n",
      "0       0              0        0                0      ...               0   \n",
      "1       0              0        0                0      ...               0   \n",
      "2       0              0        0                0      ...               0   \n",
      "3       0              0        0                0      ...               0   \n",
      "4       0              0        0                0      ...               0   \n",
      "5       0              0        0                0      ...               0   \n",
      "6       0              0        0                0      ...               0   \n",
      "7       0              0        0                0      ...               0   \n",
      "8       0              0        0                0      ...               0   \n",
      "9       0              0        0                0      ...               0   \n",
      "10      0              0        0                0      ...               0   \n",
      "11      0              0        0                0      ...               0   \n",
      "12      0              0        0                0      ...               0   \n",
      "13      0              0        0                0      ...               0   \n",
      "14      0              0        0                0      ...               0   \n",
      "15      0              0        0                0      ...               0   \n",
      "16      0              0        0                0      ...               0   \n",
      "17      0              0        0                0      ...               0   \n",
      "18      0              0        0                0      ...               0   \n",
      "19      0              0        0                0      ...               0   \n",
      "20      0              0        0                0      ...               0   \n",
      "21      0              0        0                0      ...               0   \n",
      "22      0              0        0                0      ...               0   \n",
      "23      0              0        0                0      ...               0   \n",
      "24      0              0        0                0      ...               0   \n",
      "25      0              0        0                0      ...               0   \n",
      "26      0              0        0                0      ...               0   \n",
      "27      0              0        0                0      ...               0   \n",
      "28      0              0        0                0      ...               0   \n",
      "29      0              0        0                0      ...               0   \n",
      "30      0              0        0                0      ...               0   \n",
      "31      0              0        0                0      ...               0   \n",
      "32      0              0        0                0      ...               0   \n",
      "33      0              0        0                0      ...               0   \n",
      "34      0              0        0                0      ...               0   \n",
      "35      0              0        0                0      ...               0   \n",
      "36      0              0        0                0      ...               0   \n",
      "37      0              0        0                0      ...               0   \n",
      "38      0              0        0                0      ...               0   \n",
      "39      0              0        0                0      ...               0   \n",
      "40      0              0        0                0      ...               0   \n",
      "41      0              0        0                0      ...               0   \n",
      "42      0              0        0                0      ...               0   \n",
      "43      0              0        0                0      ...               0   \n",
      "\n",
      "    StrongSub_Context  WeakSub  WeakSub_Context  Polarity  Positive  \\\n",
      "0                   0        0                0         0         0   \n",
      "1                   0        0                0         0         0   \n",
      "2                   0        0                0         0         0   \n",
      "3                   0        0                0         0         0   \n",
      "4                   0        0                0         0         0   \n",
      "5                   0        0                0         0         0   \n",
      "6                   0        0                0         0         0   \n",
      "7                   0        0                0         0         0   \n",
      "8                   0        0                0         0         0   \n",
      "9                   0        0                0         0         0   \n",
      "10                  0        0                0         0         0   \n",
      "11                  0        0                0         0         0   \n",
      "12                  0        0                0         0         0   \n",
      "13                  0        0                0         0         0   \n",
      "14                  0        0                0         0         0   \n",
      "15                  0        0                0         0         0   \n",
      "16                  0        0                0         0         0   \n",
      "17                  0        0                0         0         0   \n",
      "18                  0        0                0         0         0   \n",
      "19                  0        0                0         0         0   \n",
      "20                  0        0                0         0         0   \n",
      "21                  0        0                0         0         0   \n",
      "22                  0        0                0         0         0   \n",
      "23                  0        0                0         0         0   \n",
      "24                  0        0                0         0         0   \n",
      "25                  0        0                0         0         0   \n",
      "26                  0        0                0         0         0   \n",
      "27                  0        0                0         0         0   \n",
      "28                  0        0                0         0         0   \n",
      "29                  0        0                0         0         0   \n",
      "30                  0        0                0         0         0   \n",
      "31                  0        0                0         0         0   \n",
      "32                  0        0                0         0         0   \n",
      "33                  0        0                0         0         0   \n",
      "34                  0        0                0         0         0   \n",
      "35                  0        0                0         0         0   \n",
      "36                  0        0                0         0         0   \n",
      "37                  0        0                0         0         0   \n",
      "38                  0        0                0         0         0   \n",
      "39                  0        0                0         0         0   \n",
      "40                  0        0                0         0         0   \n",
      "41                  0        0                0         0         0   \n",
      "42                  0        0                0         0         0   \n",
      "43                  0        0                0         0         0   \n",
      "\n",
      "    Positive_Context  Negative  Negative_Context  Bias_Lexicon  \n",
      "0                  0         0                 0             0  \n",
      "1                  0         0                 0             0  \n",
      "2                  0         0                 0             0  \n",
      "3                  0         0                 0             0  \n",
      "4                  0         0                 0             0  \n",
      "5                  0         0                 0             0  \n",
      "6                  0         0                 0             0  \n",
      "7                  0         0                 0             0  \n",
      "8                  0         0                 0             0  \n",
      "9                  0         0                 0             0  \n",
      "10                 0         0                 0             0  \n",
      "11                 0         0                 0             0  \n",
      "12                 0         0                 0             0  \n",
      "13                 0         0                 0             0  \n",
      "14                 0         0                 0             0  \n",
      "15                 0         0                 0             0  \n",
      "16                 0         0                 0             0  \n",
      "17                 0         0                 0             0  \n",
      "18                 0         0                 0             0  \n",
      "19                 0         0                 0             0  \n",
      "20                 0         0                 0             0  \n",
      "21                 0         0                 0             0  \n",
      "22                 0         0                 0             0  \n",
      "23                 0         0                 0             0  \n",
      "24                 0         0                 0             0  \n",
      "25                 0         0                 0             0  \n",
      "26                 0         0                 0             0  \n",
      "27                 0         0                 0             0  \n",
      "28                 0         0                 0             0  \n",
      "29                 0         0                 0             0  \n",
      "30                 0         0                 0             0  \n",
      "31                 0         0                 0             0  \n",
      "32                 0         0                 0             0  \n",
      "33                 0         0                 0             0  \n",
      "34                 0         0                 0             0  \n",
      "35                 0         0                 0             0  \n",
      "36                 0         0                 0             0  \n",
      "37                 0         0                 0             0  \n",
      "38                 0         0                 0             0  \n",
      "39                 0         0                 0             0  \n",
      "40                 0         0                 0             0  \n",
      "41                 0         0                 0             0  \n",
      "42                 0         0                 0             0  \n",
      "43                 0         0                 0             0  \n",
      "\n",
      "[44 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "## Sample Call to the function\n",
    "\n",
    "Sent_DF = Get_Sent_Linguistic_Features(\"it was rather unfortunate that he vehemently opposed the budding indian scientist subrahmanyan chandrasekhar about his theory on the maximum mass of stars known as white dwarfs, the mass above which the star collapses and becomes a neutron star, quark star or black hole.\")\n",
    "print(Sent_DF.shape)\n",
    "print(Sent_DF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
