{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Wiki_LiguisticFeatures_Train.csv\")\n",
    "test = pd.read_csv(\"Wiki_LiguisticFeatures_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === TRAIN DATA ===\n",
      "(510467, 29)\n",
      " === TEST DATA ===\n",
      "(199992, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\" === TRAIN DATA ===\")\n",
    "print(train.shape)\n",
    "print(\" === TEST DATA ===\")\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context free features used\n",
    "# columns = ['POS', 'Hedge', 'Factive', 'Assertive',\n",
    "#        'Implicative', 'Report',\n",
    "#        'Entailment', 'StrongSub',\n",
    "#        'WeakSub', 'Polarity',\n",
    "#        'Positive', 'Negative',\n",
    "#        'Bias_Lexicon']\n",
    "\n",
    "# x_train = train[columns]\n",
    "# x_test = test[columns]\n",
    "\n",
    "x_train = train.iloc[:,2:28]\n",
    "x_test = test.iloc[:,2:28]\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:  (510467, 26)  Train Y:  (510467,)\n",
      "Test X:  (199992, 26)  Test Y:  (199992,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X: \",x_train.shape, \" Train Y: \",y_train.shape)\n",
    "print(\"Test X: \",x_test.shape, \" Test Y: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR():\n",
    "    filename = \"LR_Task2_TrainedModel.pkl\"\n",
    "    #Class Weights \n",
    "    w = {0:40, 1:60}\n",
    "    logisticRegr = LogisticRegression(class_weight=w)\n",
    "    logisticRegr.fit(x_train, y_train)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(logisticRegr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LR():\n",
    "    with open('LR_Task2_TrainedModel.pkl', 'rb') as f:\n",
    "        logisticRegr = pickle.load(f)\n",
    "    predictions = logisticRegr.predict(x_test)\n",
    "    score = logisticRegr.score(x_test, y_test)\n",
    "    print(\"Accuracy :\",score*100)\n",
    "    matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "    print(\"========= CONFUSION MATRIX =========\")\n",
    "    print(matrix)\n",
    "    print(\"================ CLASSIFICATION REPORT ===============\")\n",
    "    classes=['Objective/0','Subjective/1']\n",
    "    print(metrics.classification_report(y_test, predictions,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 84.03786151446057\n",
      "========= CONFUSION MATRIX =========\n",
      "[[166677   2419]\n",
      " [ 29504   1392]]\n",
      "================ CLASSIFICATION REPORT ===============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Objective/0       0.85      0.99      0.91    169096\n",
      "Subjective/1       0.37      0.05      0.08     30896\n",
      "\n",
      "   micro avg       0.84      0.84      0.84    199992\n",
      "   macro avg       0.61      0.52      0.50    199992\n",
      "weighted avg       0.77      0.84      0.78    199992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LR(sentence):\n",
    "    subjectivity_scores = dict()\n",
    "    words = sentence['word']\n",
    "    features = sentence.iloc[:,1:27]\n",
    "    with open('LR_Task2_TrainedModel.pkl', 'rb') as f:\n",
    "        logisticRegr = pickle.load(f)\n",
    "    predictions = logisticRegr.predict_proba(x_test)\n",
    "    for i in range(0,len(words)):\n",
    "        subjectivity_scores[words[i]] = predictions[i][1]\n",
    "    return subjectivity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jim': 0.1345213103620577, 'makes': 0.15760014545373743, 'best': 0.1578157431297637, 'cake': 0.15221833688781594}\n"
     ]
    }
   ],
   "source": [
    "sent_emb = []\n",
    "sent_emb.append({'word':'jim','POS':11, 'POS_Prev':31, 'POS_Next':26, 'Sent_Position':0, 'Hedge':0, 'Hedge_Context':0, 'Factive':0, 'Factive_Context':0, 'Assertive':0, 'Assertive_Context':0, 'Implicative':0, 'Implicative_Context':0, 'Report':0, 'Report_Context':0, 'Entailment':0, 'Entailment_Context':0, 'StrongSub':0, 'StrongSub_Context':0, 'WeakSub':0, 'WeakSub_Context':0, 'Polarity':2, 'Positive':0, 'Positive_Context':0, 'Negative':0, 'Negative_Context':0, 'Bias_Lexicon':0})\n",
    "sent_emb.append({'word':'makes','POS':30, 'POS_Prev':12, 'POS_Next':6, 'Sent_Position':0, 'Hedge':0, 'Hedge_Context':0, 'Factive':0, 'Factive_Context':0, 'Assertive':0, 'Assertive_Context':0, 'Implicative':0, 'Implicative_Context':0, 'Report':0, 'Report_Context':0, 'Entailment':0, 'Entailment_Context':0, 'StrongSub':0, 'StrongSub_Context':0, 'WeakSub':0, 'WeakSub_Context':0, 'Polarity':2, 'Positive':0, 'Positive_Context':0, 'Negative':0, 'Negative_Context':0, 'Bias_Lexicon':0})\n",
    "sent_emb.append({'word':'best','POS':8, 'POS_Prev':27, 'POS_Next':11, 'Sent_Position':1, 'Hedge':0, 'Hedge_Context':0, 'Factive':0, 'Factive_Context':0, 'Assertive':0, 'Assertive_Context':0, 'Implicative':0, 'Implicative_Context':0, 'Report':0, 'Report_Context':0, 'Entailment':0, 'Entailment_Context':0, 'StrongSub':0, 'StrongSub_Context':0, 'WeakSub':0, 'WeakSub_Context':0, 'Polarity':1, 'Positive':0, 'Positive_Context':0, 'Negative':0, 'Negative_Context':0, 'Bias_Lexicon':0})\n",
    "sent_emb.append({'word':'cake','POS':11, 'POS_Prev':11, 'POS_Next':11, 'Sent_Position':1, 'Hedge':0, 'Hedge_Context':0, 'Factive':0, 'Factive_Context':0, 'Assertive':0, 'Assertive_Context':0, 'Implicative':0, 'Implicative_Context':0, 'Report':0, 'Report_Context':0, 'Entailment':0, 'Entailment_Context':0, 'StrongSub':0, 'StrongSub_Context':0, 'WeakSub':0, 'WeakSub_Context':0, 'Polarity':2, 'Positive':0, 'Positive_Context':0, 'Negative':0, 'Negative_Context':0, 'Bias_Lexicon':0})\n",
    "test_df = pd.DataFrame(sent_emb)\n",
    "ss = predict_LR(test_df)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
